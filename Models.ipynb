{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqX4EZck+8UBY3BkSxzJil"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vbRsSlwH85qp"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["!pip install allennlp"],"metadata":{"id":"pYr05JjTJrNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import AutoModel\n","from transformers import AutoTokenizer"],"metadata":{"id":"BStvbiK-H-Mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","\n","    def __init__(self, pretrained_model_name, n_class, emb_dim=768, dropout=0.5):\n","        \"\"\"\n","        Define the BERT-based model for finetuning on specific datasets.\n","\n","        Args:\n","            pretrained_model_name (str): name of the pretrained model in the huggingface library model card. Possible options: roberta-base, bert-base-uncased, albert-base-v2, etc.\n","            n_class (int): defines the number of classes for the classification task.\n","            emb_dim (int): defines the emb_dim dimension for the last linear layer, this corresponds to the config of the actual pretrained model. Default is 768.\n","            dropout (float): defines the dropout rate for the last linear layer. Default is 0.5.\n","\n","        \"\"\"\n","\n","        super(RobertaClassifier, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(pretrained_model_name) # OPTIONS: roberta-base, bert-base-uncased, albert-base-v2, etc.\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(emb_dim, n_class)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input_id, mask):\n","        \"\"\"\n","        Forward the BERT-based model for finetuning on specific datasets.\n","\n","        Args:\n","            input_id (torch.Tensor): input ids for the model, obtained from the corresponding pretrained tokenizer.\n","            mask (torch.Tensor): attention mask for the model, obtained from the corresponding pretrained tokenizer.\n","\n","        Returns:\n","            final_layer (torch.Tensor): the final layer prediction of the model, used for calculating the loss and the accuracy.\n","            attention (torch.Tensor): attention weights for each token in the input sequence.\n","            pooled_output (torch.Tensor): the pooled output of the last layer of the BERT model, used for feature extraction for interpretation.\n","\n","        \"\"\"\n","\n","        _, pooled_output,attention = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False,output_attentions=True)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        final_layer = self.relu(linear_output)\n","\n","        return final_layer,attention,pooled_output"],"metadata":{"id":"OuOBN97oIDyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class XLNetClassifier(nn.Module):\n","\n","    def __init__(self, n_class, emb_dim=768, dropout=0.5):\n","        \"\"\"\n","        Define the XLNet model for finetuning on specific datasets.\n","\n","        Args:\n","            n_class (int): defines the number of classes for the classification task.\n","            emb_dim (int): defines the emb_dim dimension for the last linear layer, this corresponds to the config of the actual pretrained model. Default is 768.\n","            dropout (float): defines the dropout rate for the last linear layer. Default is 0.5.\n","\n","        \"\"\"\n","\n","        super(XLNetClassifier, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained('xlnet-base-cased')\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, n_class)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input_id, mask):\n","        \"\"\"\n","        Forward the XLNet model for finetuning on specific datasets.\n","\n","        Args:\n","            input_id (torch.Tensor): input ids for the model, obtained from the corresponding pretrained tokenizer.\n","            mask (torch.Tensor): attention mask for the model, obtained from the corresponding pretrained tokenizer.\n","\n","        Returns:\n","            final_layer (torch.Tensor): the final layer prediction of the model, used for calculating the loss and the accuracy.\n","            attention (torch.Tensor): attention weights for each token in the input sequence.\n","            pooled_output (torch.Tensor): the pooled output of the last layer of the XLNet model, used for feature extraction for interpretation.\n","\n","        \"\"\"\n","\n","        bert_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False,output_attentions=True)\n","        output = bert_output[0]\n","        attention = bert_output[-1]\n","        pooled_output = torch.squeeze(output[:,-1,:])\n","\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        final_layer = self.relu(linear_output)\n","\n","        return final_layer,attention,pooled_output"],"metadata":{"id":"3VeLgG0YJQ7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from allennlp.modules.elmo import Elmo, batch_to_ids\n","class ELMoClassifier(nn.Module):\n","\n","    def __init__(self, n_class, dropout=0.5):\n","        \"\"\"\n","        Define the ELMo model for finetuning on specific datasets.\n","\n","        Args:\n","            n_class (int): defines the number of classes for the classification task.\n","            dropout (float): defines the dropout rate for the last linear layer. Default is 0.5.\n","\n","        \"\"\"\n","\n","        super(ELMoClassifier, self).__init__()\n","        options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n","        weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n","\n","        # Compute two different representation for each token.\n","        # Each representation is a linear weighted combination for the\n","        # 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n","\n","        self.bert = Elmo(options_file, weight_file, 2, dropout=0)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(1024, n_class)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, sentences):\n","        \"\"\"\n","        Forward the ELMo model for finetuning on specific datasets.\n","\n","        Args:\n","            sentences (List[Str]): list of sentences for the model as the input\n","\n","        Returns:\n","            final_layer (torch.Tensor): the final layer prediction of the model, used for calculating the loss and the accuracy.\n","            _: arbitrary placeholder for matchting the output format of other predifined finetuned models.\n","            pooled_output (torch.Tensor): the pooled output of the last layer of the ELMo model, used for feature extraction for interpretation.\n","\n","        \"\"\"\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        character_ids = batch_to_ids(sentences).to(device)\n","        embeddings = self.bert(character_ids)\n","        pooled_output = torch.mean(embeddings['elmo_representations'][1].to(device),dim=1)\n","        # print(len(self.bert(input_ids= input_id, attention_mask=mask,return_dict=False,output_attentions=True)))\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        final_layer = self.relu(linear_output)\n","\n","        return final_layer, _, pooled_output"],"metadata":{"id":"fm1YwHz3Juew"},"execution_count":null,"outputs":[]}]}