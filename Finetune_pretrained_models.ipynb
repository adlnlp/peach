{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMwt2CWJrixv6BHmPIGnL7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7AU04Hal9FnI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n"]},{"cell_type":"markdown","source":["# Model training and evaluation"],"metadata":{"id":"thgPmy1aAGJB"}},{"cell_type":"code","source":["def train(model, tokenzier, X_train, X_val, y_train, y_val, output_path_prefix, batch_size = 32, learning_rate = 5e-5, total_epoch = 10, max_length = 128):\n","    \"\"\"\n","    Finetune the pretrained model on the given dataset\n","\n","    Args:\n","        model(object): initialised pretrained model with self defined classification head.\n","        tokenzier(AutoTokenizer): tokenizer used by the selected pretrained model to tokenize the input text.\n","        X_train(List[Str]): list of training texts.\n","        X_val(List[Str]): list of validation texts.\n","        y_train(Array[Int]): encoded labels of training texts as NumPy array.\n","        y_val(list): encoded labels of validation texts as NumPy array.\n","        output_path_prefix(str): prefix of the path to save the trained model, comprised of the path to the output folder and the experiment name. This will be appended with '_best.pt' and '_best_state_dict.pt' to save the best model during training and its state dictionary.\n","        batch_size(int): batch size for training. Default is 32.\n","        learning_rate(float): learning rate for training. Default is 5e-5.\n","        total_epoch(int): total number of epochs for training. Default is 10.\n","        max_length(int): maximum length for the input text. Longer texts will be truncated and shorter texts will be padded with special tokens. Default is 128.\n","\n","\n","    Returns:\n","        paths(Tuple(Str)): file paths where the best model and its state dictionary is saved .\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, eps = 1e-8)\n","\n","    best = 0\n","    # Training the model\n","    for epoch in range(total_epoch):\n","        train_loss = 0\n","        for batch_start_index in range(0,len(X_train), batch_size):\n","            batch_tokenized_docs = X_train[batch_start_index:(batch_start_index+batch_size)]\n","            batch_tensor = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['input_ids']) for doc in batch_tokenized_docs], dim=0).to(device)\n","            batch_mask = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['attention_mask']) for doc in batch_tokenized_docs]).to(device)\n","            batch_labels = torch.tensor(y_train[batch_start_index:(batch_start_index+batch_size)]).to(device)\n","\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs,_,_ = model(batch_tensor,batch_mask)\n","            loss = criterion(outputs, batch_labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # validation\n","        model.eval()\n","        val_predictions = []\n","        for batch_start_index in range(0,len(X_val), batch_size):\n","\n","            batch_tokenized_docs = X_val[batch_start_index:(batch_start_index+batch_size)]\n","            batch_tensor = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['input_ids']) for doc in batch_tokenized_docs], dim=0).to(device)\n","            batch_mask = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['attention_mask']) for doc in batch_tokenized_docs]).to(device)\n","            outputs,_,_ = model(batch_tensor, batch_mask)\n","            predicted = torch.argmax(outputs, 1)\n","            val_predictions.append(predicted)\n","\n","        predictions_tensor = torch.hstack(val_predictions)\n","        current_val_acc = accuracy_score(y_val, predictions_tensor.cpu().numpy())\n","\n","\n","        # save best\n","        if current_val_acc > best:\n","            torch.save(model, f'{output_path_prefix}_best.pt')\n","            torch.save(model.state_dict(),f'{output_path_prefix}_best_state_dict.pt')\n","            best =  current_val_acc\n","        print('Epoch: %d, train loss: %.5f, val acc: %.5f, best acc: %.5f'%(epoch + 1, train_loss, current_val_acc, best))\n","\n","\n","    print('Finished Training')\n","    return f'{output_path_prefix}_best.pt', f'{output_path_prefix}_best_state_dict.pt'\n"],"metadata":{"id":"i14h9FFhALNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model_path, tokenzier, raw_texts_test, label_encoded_test, batch_size=32, max_length=128):\n","    \"\"\"\n","    Run inference of the finetuned model on the testing test features and compute the accuracy and f1 score\n","\n","    Args:\n","        model_path(str): file path where the best finetuned model is saved .\n","        tokenzier(AutoTokenizer): tokenizer used by the selected pretrained model to tokenize the input text.\n","        raw_texts_test(List[Str]): list of testing texts.\n","        label_encoded_test(Array[Int]): encoded labels of testing texts as NumPy array.\n","        batch_size(int): batch size for inference. Default is 32.\n","        max_length(int): maximum length for the input text. Longer texts will be truncated and shorter texts will be padded with special tokens. Default is 128.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    model = torch.load(model_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # evaluation on the test set\n","    model.eval()\n","    predictions = []\n","    for batch_start_index in range(0,len(raw_texts_test), batch_size):\n","\n","        batch_tokenized_docs = raw_texts_test[batch_start_index:(batch_start_index+batch_size)]\n","        batch_tensor = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['input_ids']) for doc in batch_tokenized_docs], dim=0).to(device)\n","        batch_mask = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['attention_mask']) for doc in batch_tokenized_docs]).to(device)\n","        outputs,_,_ = model(batch_tensor, batch_mask)\n","        predicted = torch.argmax(outputs, 1)\n","        predictions.append(predicted)\n","\n","    predictions_tensor = torch.hstack(predictions)\n","    print(classification_report(label_encoded_test, predictions_tensor.cpu().numpy(),digits=4))"],"metadata":{"id":"FLsOUFKLL91G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# embedding extraction and storing"],"metadata":{"id":"yqn0RJGQ3V8I"}},{"cell_type":"code","source":["def get_CLS_row_values(model_path, raw_docs, labels,max_length=128):\n","    \"\"\"\n","    Extract the CLS embeddings of the given texts.\n","\n","    Args:\n","        model_path(str): file path where the best finetuned model is saved .\n","        raw_docs(List[Str]): list of texts.\n","        labels(Array[Int]): encoded labels of texts as NumPy array.\n","        max_length(int): maximum length for the input text. Longer texts will be truncated and shorter texts will be padded with special tokens. Default is 128.\n","\n","    Returns:\n","        row_values (List[List[float]]): CLS embeddings + encoded label of the given texts.\n","    \"\"\"\n","    model = torch.load(model_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    row_values = []\n","    with torch.no_grad():\n","        model.eval()\n","        for batch_start_index in range(0,len(raw_docs), 1):\n","            batch_tokenized_docs = raw_docs[batch_start_index:(batch_start_index+1)]\n","            batch_tensor = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['input_ids']) for doc in batch_tokenized_docs], dim=0).to(device)\n","            batch_mask = torch.cat([torch.tensor(tokenizer.encode_plus(doc, add_special_tokens = True,max_length = max_length, pad_to_max_length = True,return_attention_mask = True,return_tensors = 'pt')['attention_mask']) for doc in batch_tokenized_docs]).to(device)\n","            _,_,cls_rep = model(batch_tensor,batch_mask)\n","            one_row = cls_rep.cpu().squeeze().tolist()\n","            one_row.append(labels[batch_start_index])\n","            row_values.append(one_row)\n","    return row_values"],"metadata":{"id":"R7jH5-gP-PRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_CLS_embs(model_path, raw_docs, labels, output_path, dim_size=768, max_length=128, save_csv=True):\n","    \"\"\"\n","    Convert any extracted embeddings as a dataframe and save them in CSV file.\n","\n","    Args:\n","        model_path(str): file path where the best finetuned model is saved .\n","        raw_docs(List[Str]): list of texts.\n","        labels(Array[Int]): encoded labels of texts as NumPy array.\n","        output_path(str): path to save the CSV file.\n","        dim_size(int): dimension size of the embeddings. Default is 768.\n","        max_length(int): maximum length for the input text. Longer texts will be truncated and shorter texts will be padded with special tokens. Default is 128.\n","        save_csv(bool): whether to save the CSV file or not. Default is True.\n","\n","    Returns:\n","        df (DataFrame): Given embeddings coverted to the dataframe, where each dim of the embedding is a column, and named as f'vec_val_{dim_index}', and the encoded label information is stored as the last column named as  'Class_label'.\n","    \"\"\"\n","    column_names = []\n","    for i in range(dim_size):\n","        column_names.append(\"vec_val_\" + str(i))\n","    column_names.append(\"Class_label\")\n","\n","    train_row_values = get_CLS_row_values(model_path, raw_docs, labels, max_length)\n","    df = pd.DataFrame(train_row_values, columns=column_names)\n","    if save_csv:\n","        df.to_csv(output_path,sep=',',index=False)\n","    return df\n","\n"],"metadata":{"id":"33Mi-qqo-gja"},"execution_count":null,"outputs":[]}]}