{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1J00VFAO2-e0rgDQV3Y4ZUAVTY0STs38x","timestamp":1717114873790}],"toc_visible":true,"authorship_tag":"ABX9TyM+P1Yj74GBLzVAOA9UwRtk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p-kK-VbLrQD1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702228952282,"user_tz":-660,"elapsed":24726,"user":{"displayName":"Feiqi Cao","userId":"11387377942967144783"}},"outputId":"c92d8e83-829f-4c7c-f6d6-364467e16ec1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# TF-IDF extraction"],"metadata":{"id":"nQNg0zSvXl7b"}},{"cell_type":"code","source":["from collections import Counter\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","\n","def tfidf_extract(raw_texts_train):\n","    \"\"\"\n","    Calcuate TFIDF for the input texts\n","\n","    Args:\n","        raw_texts_train (List[Str]): training corpus documents\n","\n","    Returns:\n","        tfidf_df (Dataframe): TFIDF matrix for the word tokens in the training corpus\n","        cleaned_docs (List[Str]): cleaned training corpus documents\n","    \"\"\"\n","    cleaned_docs = [re.sub(\"[.,!?:;-='...'@#_]\", \" \", doc) for doc in raw_texts_train]\n","    cleaned_docs = [re.sub(r'\\d+', '', doc) for doc in cleaned_docs]\n","    cleaned_docs = [doc.lower() for doc in cleaned_docs]\n","\n","    tfidfvectorizer = TfidfVectorizer(analyzer= 'word', stop_words='english')\n","    tfidf_wm = tfidfvectorizer.fit_transform(cleaned_docs)\n","    tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n","    dense = tfidf_wm.todense()\n","    lst1 = dense.tolist()\n","    tfidf_df = pd.DataFrame(lst1, columns=tfidf_tokens)\n","    return tfidf_df, cleaned_docs"],"metadata":{"id":"4QUgqi-BXq21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# POS, NER extraction"],"metadata":{"id":"dOJ_FeCRX8t5"}},{"cell_type":"code","source":["import spacy\n","def pos_ner_extract(tfidf_df, cleaned_docs, spacy_model=\"en_core_web_sm\"):\n","    \"\"\"\n","    Compute POS and NER for the input texts\n","\n","    Args:\n","        tfidf_df (Dataframe): TFIDF matrix for the word tokens in the training corpus\n","        cleaned_docs (List[Str]): cleaned training corpus documents\n","        spacy_model (str, optional): spacy model name, default as 'en_core_web_sm'\n","\n","    Returns:\n","        pos_df (Dataframe): possible POS tags for the word tokens in the training corpus context\n","        ner_df (Dataframe): possible NEG tags for the word tokens in the training corpus context\n","    \"\"\"\n","    nlp = spacy.load(spacy_model)\n","\n","    pos_df = pd.DataFrame('-', columns=tfidf_df.columns, index=tfidf_df.index)\n","    ner_df = pd.DataFrame('-', columns=tfidf_df.columns, index=tfidf_df.index)\n","    for i, text in enumerate(cleaned_docs):\n","        doc = nlp(text)\n","        for token in doc:\n","            if token.text in tfidf_df.columns:\n","                pos_df.loc[i,token.text] = token.tag_\n","                if token.ent_type_ in [\"PERSON\", \"ORG\", \"GPE\",\"LOC\"]:\n","                    ner_df.loc[i,token.text] = token.ent_type_\n","                elif token.ent_iob_ != \"O\":\n","                    ner_df.loc[i,token.text] = \"MISC\"\n","    return pos_df, ner_df"],"metadata":{"id":"HuRo6fDQX7rE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make fired rule extraction script"],"metadata":{"id":"0m7YpQ78wkcX"}},{"cell_type":"code","source":["def make_fired_rule_script(dt_folder_path):\n","    \"\"\"\n","    Generate the fired rule extraction script based on the decision tree rules\n","\n","    Args:\n","        dt_folder_path (str): path where the decision tree python script is saved, and where the fired rule extraction script will be saved\n","\n","    Returns:\n","        branch_dict (dict), where each key is a (parent, child, edge) triplet for the decision tree structure, and the value is an empty list to store the document index belonging to the child node.\n","        edges_differentiated (List[List[Str]]), which saves the [parent, child]\n","        edge_labels_differentiated (dict), where each key is a (parent, child) tuple, and the value is the edge label of the tuple\n","    \"\"\"\n","    rule_script_path = f\"/{dt_folder_path}/outputs/rules/rules.py\"\n","    fired_rule_script_path = f\"/{dt_folder_path}/outputs/rules/rule_extraction.py\"\n","\n","    tree_file = open(rule_script_path,'r')\n","    content = tree_file.read()\n","    tree_file.close()\n","\n","    content_by_line = content.split('\\n')\n","    cleaned_content = []\n","    for i,line in enumerate(content_by_line):\n","        if line == \"\\n\":\n","            continue\n","        if 'def' in line:\n","            continue\n","        if 'else' in line:\n","            continue\n","        if '#' in line:\n","            continue\n","        if 'return' in line and 'else' in content_by_line[i-1]:\n","            continue\n","        cleaned_content.append(line)\n","\n","    if cleaned_content[1].count('\\t') - cleaned_content[0].count('\\t') == 1:\n","        tab = '\\t'\n","        used_original_tab = True\n","    else:\n","        num_space_per_tab = cleaned_content[1].count(' ') - cleaned_content[0].count(' ')\n","        tab = ' ' * num_space_per_tab\n","        used_original_tab = False\n","\n","    # add indentation for original lines\n","    updated_cleaned_content = []\n","    for line in cleaned_content:\n","        new_line = tab+line\n","        updated_cleaned_content.append(new_line)\n","\n","    # prepare output py content lines\n","    output_lines = []\n","    output_lines.append(\"def extraction(X_test,branch_dict):\")\n","    output_lines.append(tab+\"instance_no = len(X_test.index)\")\n","    output_lines.append(tab+\"for i in range(instance_no):\")\n","    output_lines.append(tab*2+\"obj = X_test.iloc[i]\")\n","\n","\n","    # get all the node and edges of the DT in text\n","    leaf_count = 0\n","    edges, edge_labels = [], {}\n","    parent_counter = {}\n","    edges_differentiated = []\n","    edge_labels_differentiated = {}\n","    branch_dict = {} # initialize branch dict to contain the docs belonging to each branch,\n","    for i, line in enumerate(cleaned_content):\n","        if 'if' in line and 'if' in cleaned_content[i+1]:\n","            # print(line.split('if ')[1])\n","            parent_no = line.split('if ')[1].split(']')[0].split('[')[1]\n","            parent = \"vec_val_\" + parent_no\n","            edge = line.split('if ')[1].split(']')[1].split(':')[0]\n","            child_no = cleaned_content[i+1].split('if ')[1].split(']')[0].split('[')[1]\n","            child = \"vec_val_\" + child_no\n","            edges.append([parent,child])\n","            edge_labels[(parent,child)] = edge\n","            try:\n","                parent_counter[parent] += 1\n","            except:\n","                parent_counter[parent] = 1\n","            if (parent_counter[parent]-1)//2 > 0:\n","                parent = parent+ '_' + str((parent_counter[parent]-1)//2)\n","            if child in parent_counter and (parent_counter[child])//2 > 0:\n","                child = child+ '_' + str((parent_counter[child])//2)\n","            edges_differentiated.append([parent,child])\n","            edge_labels_differentiated[(parent,child)] = edge\n","\n","            # branch dict to contain the docs belonging to each branch, and add content to output source\n","            branch_dict[(parent, child, edge)] = []\n","            output_lines.append(updated_cleaned_content[i])\n","            if used_original_tab:\n","                num_tabs_next_level = cleaned_content[i+1].count(\"\\t\")\n","            else:\n","                num_tabs_next_level = int((cleaned_content[i+1].count(\" \") - 1)/num_space_per_tab)\n","            output_lines.append(tab*(num_tabs_next_level+1) + f\"branch_dict[('{parent}','{child}','{edge}')].append(i)\")\n","\n","        if 'if' in line and 'return' in cleaned_content[i+1]:\n","            parent_no = line.split('if ')[1].split(']')[0].split('[')[1]\n","            parent = \"vec_val_\" + parent_no\n","            edge = line.split('if ')[1].split(']')[1].split(':')[0]\n","            if \"return '0'\" in cleaned_content[i+1]:\n","                child = f\"L{leaf_count}_neg\"\n","                leaf_count += 1\n","            elif \"return '1'\" in cleaned_content[i+1]:\n","                child = f\"L{leaf_count}_pos\"\n","                leaf_count += 1\n","\n","            edges.append([parent,child])\n","            edge_labels[(parent,child)] = edge\n","            try:\n","                parent_counter[parent] += 1\n","            except:\n","                parent_counter[parent] = 1\n","            if (parent_counter[parent]-1)//2 > 0:\n","                parent = parent+ '_' + str((parent_counter[parent]-1)//2)\n","            if child in parent_counter and (parent_counter[child])//2 > 0:\n","                child = child+ '_' + str((parent_counter[child])//2)\n","            edges_differentiated.append([parent,child]) # edges_differentiated[0][0] is root node\n","            edge_labels_differentiated[(parent,child)] = edge\n","\n","            branch_dict[(parent, child, edge)] = []\n","            output_lines.append(updated_cleaned_content[i])\n","            if used_original_tab:\n","                num_tabs_next_level = cleaned_content[i+1].count(\"\\t\")\n","            else:\n","                num_tabs_next_level = int((cleaned_content[i+1].count(\" \") - 1)/num_space_per_tab)\n","            output_lines.append(tab*(num_tabs_next_level+1) + f\"branch_dict[('{parent}','{child}','{edge}')].append(i)\")\n","\n","\n","    output_lines.append(tab+\"return branch_dict\")\n","\n","\n","    updated_output_lines = []\n","    for line in output_lines:\n","        updated_output_lines.append(line+'\\n')\n","\n","    out_f = open(fired_rule_script_path,'w')\n","    out_f.writelines(updated_output_lines)\n","    out_f.close()\n","\n","    return branch_dict, edges_differentiated, edge_labels_differentiated"],"metadata":{"id":"qzlzvNULZAcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# fired rule extraction for train set"],"metadata":{"id":"fNpuI8E5aCYK"}},{"cell_type":"code","source":["import os\n","\n","def fired_rule_extraction(dt_folder_path, X_train, branch_dict):\n","    \"\"\"\n","    Generate the fired rule extraction script based on the decision tree rules\n","\n","    Args:\n","        dt_folder_path (str): where the fired rule extraction script is saved\n","        X_train (Dataframe): training set features\n","        branch_dict (Dict): where fired rule information will be saved, each key is a (parent, child, edge) triplet for the decision tree structure, and the value is an empty list to store the document index belonging to the child node.\n","\n","    Returns:\n","        branch_dict (dict), where fired rule information is saved,  each key is a (parent, child, edge) triplet for the decision tree structure, and the value is a list of the document indices belonging to the child node.\n","        edges_differentiated (List[List[Str]]), which saves the [parent, child]\n","        edge_labels_differentiated (dict), where each key is a (parent, child) tuple, and the value is the edge label of the tuple\n","    \"\"\"\n","    os.chngdir(dt_folder_path)\n","    import importlib\n","    import rule_extraction\n","    importlib.reload(rule_extraction)\n","    from rule_extraction import extraction\n","    branch_dict, edges_differentiated, edge_labels_differentiated = make_fired_rule_script(dt_folder_path)\n","    branch_dict = extraction(X_train, branch_dict)\n","    return branch_dict, edges_differentiated, edge_labels_differentiated"],"metadata":{"id":"5iz0vLUqaI27"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# collate node information from fired rules"],"metadata":{"id":"TOVBQzqUatRN"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import json\n","from os import path\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","from wordcloud import STOPWORDS\n","\n","import matplotlib.pyplot as plt"],"metadata":{"id":"arnTgn3Pay_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_tree_rendering_info(tfidf_df,pos_df,ner_df,branch_dict,edges_differentiated,edge_labels_differentiated,output_path):\n","    \"\"\"\n","    Based on the tree strucutre and the training documents that each node should have, collate the top important words based on their TFIDF values, and their corresponding POS and NER tags for rendering later.\n","\n","    Args:\n","        tfidf_df (Dataframe): TFIDF matrix for the word tokens in the training corpus\n","        pos_df (Dataframe): possible POS tags for the word tokens in the training corpus context\n","        ner_df (Dataframe): possible NEG tags for the word tokens in the training corpus context\n","        branch_dict (dict), where fired rule information is saved,  each key is a (parent, child, edge) triplet for the decision tree structure, and the value is a list of the document indices belonging to the child node.\n","        edges_differentiated (List[List[Str]]), which saves the [parent, child]\n","        edge_labels_differentiated (dict), where each key is a (parent, child) tuple, and the value is the edge label of the tuple\n","        output_path (str): where the tree rendering info should be saved\n","\n","    \"\"\"\n","    FINAL_JSON = []\n","    nodes_info = []\n","\n","    added_stopwords = ['s', 'S', 'said', 'will', 'u','movie','film']\n","    sw = STOPWORDS.update(added_stopwords)\n","    tfidf_df_word_based = tfidf_df.T.sum(axis=1).sort_values(ascending=False)\n","\n","    pos_df_word_based = pos_df.T\n","    ner_df_word_based = ner_df.T\n","    for word in STOPWORDS:\n","        if word in tfidf_df_word_based.index:\n","            tfidf_df_word_based=tfidf_df_word_based.drop(word)\n","            pos_df_word_based = pos_df_word_based.drop(word)\n","            ner_df_word_based = ner_df_word_based.drop(word)\n","\n","    # deal with root node\n","    node ={\"node_name\":edges_differentiated[0][0],\n","        \"contain_words\":[],\n","        \"children\":[]}\n","\n","    # add words info for current node\n","    for word in tfidf_df_word_based.index[:100]:\n","        importance = float(tfidf_df_word_based.loc[word])\n","        pos = set(pos_df_word_based.loc[word].values.tolist())\n","        pos = [item for item in list(pos) if item != '-']\n","        # print(pos)\n","        ner = set(ner_df_word_based.loc[word].values.tolist())\n","        ner = [item for item in list(ner) if item != '-']\n","        current_word_dict = {\n","            \"text\": word,\n","            \"weight\": importance,\n","            \"POS\": pos,\n","            \"NER\": ner\n","        }\n","        node[\"contain_words\"].append(current_word_dict)\n","\n","    # add child info for current node\n","    for key, val in edge_labels_differentiated.items():\n","        if node[\"node_name\"] == key[0]:\n","            node[\"children\"].append({\"edge\":val,\"node_name\":key[1]})\n","\n","    nodes_info.append(node)\n","\n","\n","\n","    # visualize all other nodes except for root node\n","    for key,val in branch_dict.items():\n","        print(key)\n","        branch_tfidf_df = tfidf_df.iloc[val,:]\n","        # Create and generate a word cloud image:\n","        branch_tfidf_df_word_based = branch_tfidf_df.T.sum(axis=1).sort_values(ascending=False)\n","        branch_pos_df_word_based = pos_df.iloc[val,:].T\n","        branch_ner_df_word_based = ner_df.iloc[val,:].T\n","        for word in STOPWORDS:\n","            if word in branch_tfidf_df_word_based.index:\n","                branch_tfidf_df_word_based=branch_tfidf_df_word_based.drop(word)\n","                branch_pos_df_word_based = branch_pos_df_word_based.drop(word)\n","                branch_ner_df_word_based = branch_ner_df_word_based.drop(word)\n","\n","        # deal with current node\n","        node ={\"node_name\":key[1],\n","            \"contain_words\":[],\n","            \"children\":[]}\n","\n","        # add words info for current node\n","        for word in branch_tfidf_df_word_based.index[:100]:\n","            importance = float(branch_tfidf_df_word_based.loc[word])\n","            pos = set(branch_pos_df_word_based.loc[word].values.tolist())\n","            pos = [item for item in list(pos) if item != '-']\n","            ner = set(branch_ner_df_word_based.loc[word].values.tolist())\n","            ner = [item for item in list(ner) if item != '-']\n","            current_word_dict = {\n","                \"text\": word,\n","                \"weight\": importance,\n","                \"POS\": pos,\n","                \"NER\": ner\n","            }\n","            node[\"contain_words\"].append(current_word_dict)\n","\n","        # add child info for current node\n","        for check_key, check_val in edge_labels_differentiated.items():\n","            if node[\"node_name\"] == check_key[0]:\n","                node[\"children\"].append({\"edge\":check_val,\"node_name\":check_key[1]})\n","\n","        nodes_info.append(node)\n","\n","    FINAL_JSON.append(nodes_info)\n","    with open(f\"{output_path}.json\", \"w\") as outfile:\n","        json.dump(FINAL_JSON, outfile)\n","    with open(f\"{output_path}.js\", 'w') as f:\n","            f.write(f\"var nodes_info={FINAL_JSON};\")"],"metadata":{"id":"exOpvNZRa_sG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decision path for test set"],"metadata":{"id":"P50cvE0iObw0"}},{"cell_type":"code","source":["import os\n","\n","def decision_path_extraction(dt_folder_path, df_test, output_path):\n","    \"\"\"\n","    Based on the tree strucutre and the training documents that each node should have, collate the top important words based on their TFIDF values, and their corresponding POS and NER tags for rendering later.\n","\n","    Args:\n","        dt_folder_path (str): where the fired rule extraction script is saved\n","        df_test (Dataframe): extracted features for the test set\n","        raw_texts_test (List[Str]): test set documents\n","        tokenized_docs_test (List[List[Str]]): tokenized test set documents\n","        output_path (str): where the decision path of the each test set document should be saved.\n","\n","    \"\"\"\n","    os.chngdir(dt_folder_path)\n","    import importlib\n","    import rule_extraction\n","    importlib.reload(rule_extraction)\n","    from rule_extraction import extraction\n","    branch_dict, edges_differentiated, edge_labels_differentiated = make_fired_rule_script(dt_folder_path)\n","    no_feats = len(df_test.columns) - 1\n","    X_test = df_test.iloc[:,:no_feats]\n","    branch_dict = extraction(X_test, branch_dict)\n","\n","    all_test_doc_info = []\n","    for idx in range(len(df_test)):\n","        doc_info = {\n","            \"doc_id\": idx,\n","            \"raw_text\": raw_texts_test[idx],\n","            \"tokenized_text\": tokenized_docs_test[idx],\n","            \"decision_path\":[]\n","        }\n","        all_test_doc_info.append(doc_info)\n","\n","    for key, vals in branch_dict.items():\n","        for idx in vals:\n","            child_at_each_step = key[1]\n","            all_test_doc_info[idx][\"decision_path\"].append(\"item_\"+child_at_each_step)\n","\n","    with open(f\"{output_path}.json\", \"w\") as outfile:\n","        json.dump(all_test_doc_info, outfile)\n","    with open(f\"{output_path}.js\", 'w') as f:\n","            f.write(f\"var precomp_testset_info={all_test_doc_info};\")"],"metadata":{"id":"p9Mm4OLzOgfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# similar word look up table extraction"],"metadata":{"id":"rFfD19f3sUwe"}},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn"],"metadata":{"id":"XPecDgqT0Bch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def construct_word_synonym_dict(input_file_path, output_file_path, save_as_js=True):\n","    \"\"\"\n","    Based on WordNet knowledge, collate the synonym sets for words in the wordclouds of the decision tree.\n","\n","    Args:\n","        input_file_path (str): where the tree node words are saved. e.g. '/content/drive/My Drive/Decision_Tree/doc_emb_tree/demo_json/MR_CNN_70_roberta_depth_3.json'\n","        output_file_path (str): where the synonym look up dict should be saved. e.g. '/content/drive/My Drive/Decision_Tree/doc_emb_tree/demo_json/mr_cnn_roberta_depth_3_wn_synonyms.js'\n","        save_as_js (bool, optional): a flag to decide whether to save output as js file or not, default as True\n","\n","    Returns:\n","        word_wn_synonyms (Dict): the extracted similar word look up table, and save it as js file\n","    \"\"\"\n","    with open(file_path, 'r') as f:\n","        data = json.load(f)\n","\n","    words=[]\n","    for node_dict in data[0]:\n","        word_dicts = node_dict['contain_words']\n","        for word_dict in word_dicts:\n","            if word_dict['text'] not in words:\n","                words.append(word_dict['text'])\n","\n","    word_wn_synonyms = {}\n","    for word in words:\n","        word_wn_synonyms[word] = []\n","\n","    for word in words:\n","        for synset in wn.synsets(word):\n","            for lemma in synset.lemmas():\n","                if lemma.name() not in word_wn_synonyms[word] and lemma.name().lower() != word:\n","                    word_wn_synonyms[word].append(lemma.name().lower())\n","    if save_as_js:\n","        with open(output_file_path, 'w') as f:\n","            f.write(f\"var synsets={word_wn_synonyms};\")\n","\n","    return word_wn_synonyms"],"metadata":{"id":"PnUPzC_pySZb"},"execution_count":null,"outputs":[]}]}