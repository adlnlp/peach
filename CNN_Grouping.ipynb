{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9tbp5GPjOmPyPEZFYVTYa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TnDc09AeK1YV"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","def make_labels_from_extracted_emb(train_df, test_df):\n","    \"\"\"\n","    Encode training and testing set labels\n","\n","    Args:\n","        train_df (Dataframe): training corpus features where the last column is the label\n","        test_df (Dataframe): testing corpus features where the last column is the label\n","\n","    Returns:\n","        label_encoded_train (Array[Int]): encoded labels for training set\n","        label_encoded_test (Array[Int]): encoded labels for testing set\n","    \"\"\"\n","    no_feats = len(train_df.columns)-1\n","    lEnc = LabelEncoder()\n","    lEnc.fit(train_df.iloc[:,no_feats])\n","    label_encoded_train = lEnc.transform(train_df.iloc[:,no_feats])\n","    label_encoded_test = lEnc.transform(test_dfiloc[:,no_feats])\n","    return label_encoded_train, label_encoded_test"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"WibDOPMXMOn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"ZzxSClChMLcG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNNClassifier(nn.Module):\n","\n","    def __init__(self, input_dim= 768, reduced_dim = 10, last_k =11, last_s = 20):\n","        \"\"\"\n","        Define the CNN model for reducing the feature dimension of pretrained embeddings\n","\n","        Args:\n","            input_dim (int): defines the input embedding dimension. Default is 768.\n","            reduced_dim (int): defines the target embedding dimension. Default is 10.\n","            last_k (int): defines the kernel size of the last pooling layer. Default is 11.\n","            last_s (int): defines the stride size of the last pooling layer. Default is 20.\n","\n","        \"\"\"\n","\n","        super(CNNClassifier, self).__init__()\n","\n","        self.layer1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2)\n","        self.act1 = nn.ReLU()\n","        self.pooling1 = nn.AvgPool1d(2, stride=2)\n","\n","        self.layer2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)\n","        self.act2 = nn.ReLU()\n","        self.pooling2 = nn.AvgPool1d(last_k, stride=last_s)\n","\n","        self.linear = nn.Linear(reduced_dim, n_class)\n","\n","    def forward(self, input_feature):\n","        \"\"\"\n","        Forward the CNN model for reducing the feature dimension and classification\n","\n","        \"\"\"\n","\n","\n","        pooled_output = self.pooling1(self.act1(self.layer1(input_feature)))\n","        pooled_output = self.pooling2(self.act2(self.layer2(pooled_output)))\n","\n","        prediction_logit = self.linear(pooled_output)\n","\n","        return prediction_logit,pooled_output"],"metadata":{"id":"NFoV7t8sML6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","def train(model, X_train, X_val, y_train, y_val, output_path_prefix, batch_size = 32, learning_rate = 5e-5, total_epoch = 10):\n","    \"\"\"\n","    Train the CNN model on the given features and labels\n","\n","    Args:\n","        model(object): initialised pretrained model with self defined classification head.\n","        X_train(List[Str]): list of training texts.\n","        X_val(List[Str]): list of validation texts.\n","        y_train(Array[Int]): encoded labels of training texts as NumPy array.\n","        y_val(list): encoded labels of validation texts as NumPy array.\n","        output_path_prefix(str): prefix of the path to save the trained model, comprised of the path to the output folder and the experiment name. This will be appended with '_best.pt' and '_best_state_dict.pt' to save the best model during training and its state dictionary.\n","        batch_size(int): batch size for training. Default is 32.\n","        learning_rate(float): learning rate for training. Default is 5e-5.\n","        total_epoch(int): total number of epochs for training. Default is 10.\n","\n","    Returns:\n","        paths(Tuple(Str)): file paths where the best model and its state dictionary is saved .\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, eps = 1e-8)\n","\n","    best = 0\n","    # Training the model\n","    for epoch in range(total_epoch):\n","        train_loss = 0\n","        for batch_start_index in range(0,len(X_train), batch_size):\n","            batch_df = X_train.iloc[batch_start_index:(batch_start_index+batch_size),:].astype(float)\n","            batch_tensor = torch.FloatTensor(batch_df.values).to(device)\n","            batch_tensor = torch.unsqueeze(batch_tensor,1)\n","            batch_labels = torch.tensor(y_train[batch_start_index:(batch_start_index+batch_size)]).to(device)\n","\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs,_ = model(batch_tensor)\n","            outputs= torch.squeeze(outputs,1)\n","            loss = criterion(outputs, batch_labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # validation\n","        model.eval()\n","        val_predictions = []\n","        for batch_start_index in range(0,len(X_val.index), batch_size):\n","\n","            batch_df = X_val.iloc[batch_start_index:(batch_start_index+batch_size),:].astype(float)\n","            batch_tensor = torch.FloatTensor(batch_df.values).to(device)\n","            batch_tensor = torch.unsqueeze(batch_tensor,1)\n","            batch_labels = torch.tensor(y_val[batch_start_index:(batch_start_index+batch_size)]).to(device)\n","\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs,_ = model(batch_tensor)\n","            outputs= torch.squeeze(outputs,1)\n","            predicted = torch.argmax(outputs, 1)\n","            val_predictions.append(predicted)\n","\n","        predictions_tensor = torch.hstack(val_predictions)\n","        current_val_acc = accuracy_score(y_val, predictions_tensor.cpu().numpy())\n","\n","\n","        # save best\n","        if current_val_acc > best:\n","            torch.save(model, f'{output_path_prefix}_best.pt')\n","            torch.save(model.state_dict(),f'{output_path_prefix}_best_state_dict.pt')\n","            best =  current_val_acc\n","        print('Epoch: %d, train loss: %.5f, val acc: %.5f, best acc: %.5f'%(epoch + 1, train_loss, current_val_acc, best))\n","\n","\n","    print('Finished Training')\n","    return f'{output_path_prefix}_best.pt', f'{output_path_prefix}_best_state_dict.pt'\n"],"metadata":{"id":"3Tp08NCdMgKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model_path, X_test, label_encoded_test, batch_size=32):\n","    \"\"\"\n","    Run inference of the trained CNN model on the testing test features and compute the accuracy and f1 score\n","\n","    Args:\n","        model_path(str): file path where the best CNN model is saved .\n","        X_test(Dataframe): testing set features\n","        label_encoded_test(Array[Int]): encoded labels of testing texts as NumPy array.\n","        batch_size(int): batch size for inference. Default is 32.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    model = torch.load(model_or_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # evaluation on the test set\n","    model.eval()\n","    predictions = []\n","    for batch_start_index in range(0,len(X_test.index), batch_size):\n","\n","    batch_df = X_test.iloc[batch_start_index:(batch_start_index+batch_size),:].astype(float)\n","    batch_tensor = torch.FloatTensor(batch_df.values).to(device)\n","    batch_tensor = torch.unsqueeze(batch_tensor,1)\n","    batch_labels = torch.tensor(label_encoded_test[batch_start_index:(batch_start_index+batch_size)]).to(device)\n","\n","    outputs,_ = model(batch_tensor)\n","    outputs= torch.squeeze(outputs,1)\n","    predicted = torch.argmax(outputs, 1)\n","    predictions.append(predicted)\n","\n","    predictions_tensor = torch.hstack(predictions)\n","    print(classification_report(label_encoded_test, predictions_tensor.cpu().numpy(),digits=4))"],"metadata":{"id":"FLsOUFKLL91G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_CLS_row_values(model_path, raw_feature_df, labels, dim_size):\n","    \"\"\"\n","    Extract the CLS embeddings of the given texts.\n","\n","    Args:\n","        model_path(str): file path where the best finetuned model is saved .\n","        raw_feature_df(Dataframe): input features\n","        labels(Array[Int]): encoded labels of texts as NumPy array.\n","        dim_size(int): dimension size of the embeddings.\n","\n","    Returns:\n","        row_values (List[List[float]]): CLS embeddings + encoded label of the given input.\n","    \"\"\"\n","    model = torch.load(model_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    row_values = []\n","    with torch.no_grad():\n","        model.eval()\n","        for batch_start_index in range(0,len(raw_feature_df.index), 1):\n","            batch_tensor = torch.FloatTensor(raw_feature_df.iloc[batch_start_index:(batch_start_index+1),:dim_size].values).to(device)\n","            batch_tensor = torch.unsqueeze(batch_tensor,1)\n","            _,cls_rep = model(batch_tensor)\n","            one_row = cls_rep.cpu().squeeze().tolist()\n","            one_row.append(labels[batch_start_index])\n","            row_values.append(one_row)\n","    return row_values"],"metadata":{"id":"R7jH5-gP-PRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_CLS_embs(model_path, raw_feature_df, labels, output_path, dim_size=768,save_csv=True):\n","    \"\"\"\n","    Convert any extracted embeddings as a dataframe and save them in CSV file.\n","\n","    Args:\n","        model_path(str): file path where the best finetuned model is saved .\n","        raw_feature_df(Dataframe): input features\n","        labels(Array[Int]): encoded labels of texts as NumPy array.\n","        output_path(str): path to save the CSV file.\n","        dim_size(int): dimension size of the embeddings. Default is 768.\n","        save_csv(bool): whether to save the CSV file or not. Default is True.\n","\n","    Returns:\n","        df (DataFrame): Given embeddings coverted to the dataframe, where each dim of the embedding is a column, and named as f'vec_val_{dim_index}', and the encoded label information is stored as the last column named as  'Class_label'.\n","    \"\"\"\n","    column_names = []\n","    for i in range(dim_size):\n","        column_names.append(\"vec_val_\" + str(i))\n","    column_names.append(\"Class_label\")\n","\n","    train_row_values = get_CLS_row_values(model_path, raw_feature_df, labels, dim_size)\n","    df = pd.DataFrame(train_row_values, columns=column_names)\n","    if save_csv:\n","        df.to_csv(output_path,sep=',',index=False)\n","    return df\n","\n"],"metadata":{"id":"33Mi-qqo-gja"},"execution_count":null,"outputs":[]}]}