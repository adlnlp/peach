{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGVyniE3Y2hnZedx27PUfC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"VtVidsJ7pmmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classes"],"metadata":{"id":"k3DWnaAnopAp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Nu-QMaJ8x8X"},"outputs":[],"source":["class CustomDataset(object):\n","    def __init__(self, doc_path, label_path, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the user-customised dataset instance.\n","\n","        Expected input format:\n","        Doc file: txt file where each row is a document in the whole dataset.\n","        Label file: txt file where each row contains the label information in the whole dataset, which is formated as (index + \\t + split + \\t + actual label). Order correponds to the order of the documents in the Doc txt file.\n","\n","        Args:\n","            doc_path (str), path to where all the documents in the dataset are stored.\n","            label_path (str), path to where all the labels in the dataset are stored.\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        raw_in = open(doc_path,'r')\n","        raw_texts = raw_in.read().split('\\n')\n","        label_file = open(label_path,\"r\")\n","        raw_labels = label_file.read().split('\\n')\n","        self.raw_texts = raw_texts\n","        self.raw_labels = raw_labels\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","   def all_data(self):\n","        self.raw_texts_train = []\n","        self.raw_labels_train = []\n","        self.raw_texts_test = []\n","        self.raw_labels_test = []\n","        self.label_mapping = []\n","        for i in range(len(self.raw_labels)):\n","            index = self.raw_labels[i].split('\\t')[0]\n","            split = self.raw_labels[i].split('\\t')[1]\n","            label = self.raw_labels[i].split('\\t')[2]\n","            if label not in self.label_mapping:\n","                self.label_mapping.append(label)\n","\n","            if 'train' in split:\n","                self.raw_texts_train.append(self.raw_texts[i])\n","                self.raw_labels_train.append(self.label_mapping.index(label))\n","            else:\n","                self.raw_texts_test.append(self.raw_texts[i])\n","                self.raw_labels_test.append(self.label_mapping.index(label))\n","\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val"]},{"cell_type":"code","source":["from datasets import load_dataset\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import re\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","class BBCNewsDataset(object):\n","    def __init__(self, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific BBCNews dataset used in PEACH experiment.\n","\n","        Args:\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.dataset = load_dataset(\"SetFit/bbc-news\")\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self):\n","        self.raw_texts_train = self.dataset['train']['text']\n","        self.raw_labels_train = self.dataset['train']['label']\n","        self.raw_texts_test = self.dataset['test']['text']\n","        self.raw_labels_test = self.dataset['test']['label']\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val\n","\n"],"metadata":{"id":"w7JLMOEAtEJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MRDataset(object):\n","    def __init__(self, doc_path, label_path, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific MR dataset used in PEACH experiment.\n","\n","        Args:\n","            doc_path (str), path to where all the documents in the dataset are stored.\n","            label_path (str), path to where all the labels in the dataset are stored.\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        raw_in = open(doc_path,'r')\n","        raw_texts = raw_in.read().split('\\n')\n","        label_file = open(label_path,\"r\")\n","        raw_labels = label_file.read().split('\\n')\n","        self.raw_texts = raw_texts\n","        self.raw_labels = raw_labels\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","   def all_data(self):\n","        self.raw_texts_train = []\n","        self.raw_labels_train = []\n","        self.raw_texts_test = []\n","        self.raw_labels_test = []\n","        self.label_mapping = []\n","        for i in range(len(self.raw_labels)):\n","            index = self.raw_labels[i].split('\\t')[0]\n","            split = self.raw_labels[i].split('\\t')[1]\n","            label = self.raw_labels[i].split('\\t')[2]\n","            if label not in self.label_mapping:\n","                self.label_mapping.append(label)\n","\n","            if 'train' in split:\n","                self.raw_texts_train.append(self.raw_texts[i])\n","                self.raw_labels_train.append(self.label_mapping.index(label))\n","            else:\n","                self.raw_texts_test.append(self.raw_texts[i])\n","                self.raw_labels_test.append(self.label_mapping.index(label))\n","\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            if len(lower_case)>128:\n","                lower_case = lower_case[:128]\n","            else:\n","                lower_case = lower_case\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val\n"],"metadata":{"id":"kKDvM9uWtgBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMDBDataset(object):\n","    def __init__(self, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific IMDB dataset used in PEACH experiment.\n","\n","        Args:\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.dataset = load_dataset(\"imdb\")\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self):\n","        self.raw_texts_train = self.dataset['train']['text']\n","        self.raw_labels_train = self.dataset['train']['label']\n","        self.raw_texts_test = self.dataset['test']['text']\n","        self.raw_labels_test = self.dataset['test']['label']\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val\n","\n"],"metadata":{"id":"HMslKEP6tb1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MSRPDataset(object):\n","    def __init__(self):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific MSRP dataset used in PEACH experiment.\n","\n","        Args:\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.dataset = load_dataset(\"HHousen/msrp\")\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self):\n","        raw_texts_A_train = self.dataset['train']['sentence1']\n","        raw_texts_B_train = self.dataset['train']['sentence2']\n","        self.raw_texts_train = [raw_texts_A_train[i]+ ' ' + raw_texts_B_train[i] for i in range(len(raw_texts_A_train))]\n","        self.raw_labels_train = self.dataset['train']['label']\n","\n","\n","        raw_texts_A_test = self.dataset['test']['sentence1']\n","        raw_texts_B_test = self.dataset['test']['sentence2']\n","        self.raw_texts_test = [raw_texts_A_test[i]+ ' ' + raw_texts_B_test[i] for i in range(len(raw_texts_A_test))]\n","        self.raw_labels_test = self.dataset['test']['label']\n","\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test,  self.raw_labels_train, self.raw_labels_test\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test, self.label_encoded_val\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val\n"],"metadata":{"id":"YO3FWOTTthOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SICKDataset(object):\n","    def __init__(self):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific SICK dataset used in PEACH experiment.\n","\n","        Args:\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.dataset = load_dataset(\"sick\")\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self):\n","        raw_texts_A_train = self.dataset['train']['sentence_A']\n","        raw_texts_B_train = self.dataset['train']['sentence_B']\n","        self.raw_texts_train = [raw_texts_A_train[i]+ ' ' + raw_texts_B_train[i] for i in range(len(raw_texts_A_train))]\n","        self.raw_labels_train = self.dataset['train']['label']\n","\n","        raw_texts_A_val = self.dataset['validation']['sentence_A']\n","        raw_texts_B_val =self.dataset['validation']['sentence_B']\n","        self.raw_texts_val = [raw_texts_A_val[i]+ ' ' + raw_texts_B_val[i] for i in range(len(raw_texts_A_val))]\n","        self.raw_labels_val = self.dataset['validation']['label']\n","\n","        raw_texts_A_test = self.dataset['test']['sentence_A']\n","        raw_texts_B_test = self.dataset['test']['sentence_B']\n","        self.raw_texts_test = [raw_texts_A_test[i]+ ' ' + raw_texts_B_test[i] for i in range(len(raw_texts_A_test))]\n","        self.raw_labels_test = self.dataset['test']['label']\n","\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            self.tokenized_texts_val = self.initial_clean_up(self.raw_texts_val)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.tokenized_texts_val, self.raw_labels_train, self.raw_labels_test, self.raw_labels_val\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_texts_val, self.raw_labels_train, self.raw_labels_test, self.raw_labels_val\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        self.label_encoded_val = self.lEnc.transform(self.raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test, self.label_encoded_val\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = self.raw_texts_train, self.raw_texts_val, self.label_encoded_train, self.label_encoded_val\n","        else:\n","            X_train, X_val, y_train, y_val =  self.tokenized_docs_train,  self.tokenized_docs_val,  self.label_encoded_train,  self.label_encoded_val\n","        return X_train, X_val, y_train, y_val\n"],"metadata":{"id":"vGfMDtkOtk2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","class SST2Dataset(object):\n","    def __init__(self, train_path, val_path, test_path, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific SST2 dataset used in PEACH experiment.\n","\n","        Args:\n","            train_path (str), path to where all the documents and labels in the training dataset are stored.\n","            val_path (str), path to where all the documents and labels in the validation dataset are stored.\n","            test_path (str), path to where all the documents and labels in the testing dataset are stored.\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.train_in_df = pd.read_csv('train.tsv', sep='\\t', header=None)\n","        self.val_in_df = pd.read_csv('dev.tsv', sep='\\t', header=None)\n","        self.test_in_df = pd.read_csv('test.tsv', sep='\\t', header=None)\n","        self.original_split = ['train','test''val']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self)\n","        self.raw_texts_train = self.train_in_df[0].tolist()\n","        self.raw_labels_train = self.train_in_df[1].tolist()\n","        self.raw_texts_val = self.val_in_df[0].tolist()\n","        self.raw_labels_val = self.val_in_df[1].tolist()\n","        self.raw_texts_test = self.test_in_df[0].tolist()\n","        self.raw_labels_test = self.test_in_df[1].tolist()\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            self.tokenized_texts_val = self.initial_clean_up(self.raw_texts_val)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.tokenized_texts_val, self.raw_labels_train, self.raw_labels_test, self.raw_labels_val\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_texts_val, self.raw_labels_train, self.raw_labels_test, self.raw_labels_val\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        self.label_encoded_val = self.lEnc.transform(self.raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test, self.label_encoded_val\n","\n","    def train_val_split(self):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = self.raw_texts_train, self.raw_texts_val, self.label_encoded_train, self.label_encoded_val\n","        else:\n","            X_train, X_val, y_train, y_val =  self.tokenized_docs_train,  self.tokenized_docs_val,  self.label_encoded_train,  self.label_encoded_val\n","        return X_train, X_val, y_train, y_val"],"metadata":{"id":"KdyFD1fttnJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TRECDataset(object):\n","    def __init__(self, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific TREC dataset used in PEACH experiment.\n","\n","        Args:\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        self.dataset = load_dataset(\"trec\")\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","    def all_data(self):\n","        self.raw_texts_train = self.dataset['train']['text']\n","        self.raw_labels_train = self.dataset['train']['coarse_label']\n","        self.raw_texts_test = self.dataset['test']['text']\n","        self.raw_labels_test = self.dataset['test']['coarse_label']\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(self.raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(self.raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val\n","\n","\n"],"metadata":{"id":"ML8V6dPdtozb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TwentyNGDataset(object):\n","    def __init__(self, doc_path, label_path, word_level_tokenize=False):\n","        \"\"\"\n","        Define and pre-process(e.g. split, cleanup and encode label) the specific 20ng dataset used in PEACH experiment.\n","\n","        Args:\n","            doc_path (str), path to where all the documents in the dataset are stored.\n","            label_path (str), path to where all the labels in the dataset are stored.\n","            word_level_tokenize (boolean), to indicate whether tokenized docs should be output. This is necessary for ELMo model but not other pretrained models. Default to be False\n","\n","        \"\"\"\n","        raw_in = open(doc_path,'r')\n","        raw_texts = raw_in.read().split('\\n')\n","        label_file = open(label_path,\"r\")\n","        raw_labels = label_file.read().split('\\n')\n","        self.raw_texts = raw_texts\n","        self.raw_labels = raw_labels\n","        self.original_split = ['train','test']\n","        self.word_level_tokenize = word_level_tokenize\n","\n","   def all_data(self):\n","        self.raw_texts_train = []\n","        self.raw_labels_train = []\n","        self.raw_texts_test = []\n","        self.raw_labels_test = []\n","        self.label_mapping = []\n","        for i in range(len(self.raw_labels)):\n","            index = self.raw_labels[i].split('\\t')[0]\n","            split = self.raw_labels[i].split('\\t')[1]\n","            label = self.raw_labels[i].split('\\t')[2]\n","            if label not in self.label_mapping:\n","                self.label_mapping.append(label)\n","\n","            if 'train' in split:\n","                self.raw_texts_train.append(self.raw_texts[i])\n","                self.raw_labels_train.append(self.label_mapping.index(label))\n","            else:\n","                self.raw_texts_test.append(self.raw_texts[i])\n","                self.raw_labels_test.append(self.label_mapping.index(label))\n","\n","        if self.word_level_tokenize:\n","            self.tokenized_texts_train = self.initial_clean_up(self.raw_texts_train)\n","            self.tokenized_texts_test = self.initial_clean_up(self.raw_texts_test)\n","            return self.tokenized_texts_train, self.tokenized_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","        else:\n","            return self.raw_texts_train, self.raw_texts_test, self.raw_labels_train, self.raw_labels_test, self.label_mapping\n","\n","    def initial_clean_up(self, docs):\n","        tokenized_docs=[]  # cleaned and tokenized texts\n","        for doc in docs:\n","            clean_doc = re.sub(r'[^\\w\\s]','', doc)\n","            lower_case = word_tokenize(clean_doc)\n","            if len(lower_case)>128:\n","                lower_case = lower_case[:128]\n","            else:\n","                lower_case = lower_case\n","            tokenized_docs.append(lower_case)\n","        return tokenized_docs\n","\n","    def encode_labels(self):\n","        self.lEnc = LabelEncoder()\n","        self.lEnc.fit(self.raw_labels_train)\n","        self.label_encoded_train = self.lEnc.transform(raw_labels_train)\n","        # label_encoded_val = lEnc.transform(raw_labels_val)\n","        self.label_encoded_test = self.lEnc.transform(raw_labels_test)\n","        return self.label_encoded_train, self.label_encoded_test\n","\n","    def train_val_split(self, val_size=0.1,random_state=42):\n","        if not self.word_level_tokenize:\n","            X_train, X_val, y_train, y_val = train_test_split(self.raw_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        else:\n","            X_train, X_val, y_train, y_val = train_test_split(self.tokenized_texts_train, self.label_encoded_train, test_size=val_size, random_state=random_state)\n","        return X_train, X_val, y_train, y_val"],"metadata":{"id":"Kg5vLK48tQRE"},"execution_count":null,"outputs":[]}]}